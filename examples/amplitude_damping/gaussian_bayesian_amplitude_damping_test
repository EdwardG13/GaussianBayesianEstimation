"""
Compare minimum MSL vs prior variance for Bayes-optimal S (Lyapunov) 
and linear, quadratic, and cubic ansätze for estimating damping rate Γ 
of amplitude damping channel.
"""

import numpy as np
import scipy.linalg as la
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import math

# ---------------- User parameters ----------------
N = 20            # Fock truncation
Gamma_min, Gamma_max = 0.01, 10.0  # Damping rate range (Γ > 0)
Gamma_pts = 201    # number of grid points for Γ

# Initial state parameters (before damping)
initial_state_type = 'squeezed_thermal'  # Options: 'coherent', 'thermal', 'squeezed_thermal'
alpha_coherent = 2.0  # coherent state amplitude (if coherent)
n_thermal_init = 0.5  # initial thermal photons (if thermal/squeezed_thermal)
r_squeeze_init = 0.3  # initial squeezing (if squeezed_thermal) 
phi_squeeze_init = 0.0  # squeezing phase

# Evolution time
t_evolution = 1.0  # Fixed time at which we measure

# Environment temperature
n_thermal_env = 0.0  # thermal photons in environment (σ(∞))

# Prior settings
prior_type = 'gamma'  # Options: 'gaussian', 'log_normal', 'beta', 'gamma'
Gamma0 = 0.5      # prior mean/center for Γ

# Estimand choice
estimand = 'log'  # Options: 'log' (estimate f=log(Γ)), 'identity' (estimate f=Γ)

# Range of prior variances to test
prior_variance_list = np.logspace(-2, 0.5, 12)  # σ² from 0.01 to ~3.16
# -------------------------------------------------

# Ladder operators in truncated Fock basis
a = np.zeros((N, N), dtype=complex)
for n in range(1, N):
    a[n-1, n] = np.sqrt(n)
adag = a.conj().T
I = np.eye(N, dtype=complex)

# Quadratures
x = (a + adag) / np.sqrt(2)
p = (a - adag) / (1j * np.sqrt(2))

def squeeze_op(r, phi):
    """Squeezing operator S(r,phi) = exp[r/2 * (e^{-2i*phi} a^†^2 - e^{2i*phi} a^2)]"""
    G = 0.5 * (np.exp(-2j*phi) * adag @ adag - np.exp(2j*phi) * a @ a)
    return la.expm(r * G)

def displace_op(alpha):
    """Displacement operator D(α) = exp(α a^† - α* a)"""
    return la.expm(alpha * adag - np.conj(alpha) * a)

def thermal_state(n_bar):
    """Thermal state with mean photon number n_bar"""
    if n_bar < 0:
        n_bar = 0
    rho_th = np.zeros((N, N), dtype=complex)
    for n in range(N):
        if n_bar > 0:
            rho_th[n, n] = (n_bar**n) / ((1 + n_bar)**(n+1))
        else:
            rho_th[n, n] = 1.0 if n == 0 else 0.0
    return rho_th

def initial_state(state_type, alpha=2.0, n_th=0.5, r=0.0, phi=0.0):
    """Create initial state before damping"""
    if state_type == 'coherent':
        D = displace_op(alpha)
        vac = np.zeros((N, N), dtype=complex)
        vac[0, 0] = 1.0
        rho = D @ vac @ D.conj().T
    elif state_type == 'thermal':
        rho = thermal_state(n_th)
    elif state_type == 'squeezed_thermal':
        rho_th = thermal_state(n_th)
        S = squeeze_op(r, phi)
        rho = S @ rho_th @ S.conj().T
    else:
        raise ValueError(f"Unknown state type: {state_type}")
    
    rho = 0.5 * (rho + rho.conj().T)
    rho = rho / np.trace(rho)
    return rho

def build_lindblad_superoperator(Gamma, n_th, a, adag, N):
    """Build the Lindbladian superoperator for thermal bosonic damping."""
    dim = N * N
    L_super = np.zeros((dim, dim), dtype=complex)
    I = np.eye(N, dtype=complex)
    
    def dissipator_superop(L_op):
        Ldag = L_op.conj().T
        LdagL = Ldag @ L_op
        term1 = np.kron(L_op.conj(), L_op)
        term2 = -0.5 * (np.kron(I, LdagL) + np.kron(LdagL.conj(), I))
        return term1 + term2
    
    D_a = dissipator_superop(a)
    D_adag = dissipator_superop(adag)
    L_super = Gamma * (n_th + 1) * D_a + Gamma * n_th * D_adag
    
    return L_super

def amplitude_damping_channel(rho_init, Gamma, t, n_env=0.0):
    """Apply amplitude damping channel for time t with rate Γ."""
    if n_env < 1e-10:
        eta = np.exp(-Gamma * t)
        rho_out = np.zeros((N, N), dtype=complex)
        
        for k in range(N):
            E_k = np.zeros((N, N), dtype=complex)
            for n in range(k, N):
                binom_coeff = math.factorial(n) / (math.factorial(k) * math.factorial(n - k))
                amplitude = np.sqrt(binom_coeff * eta**(n-k) * (1-eta)**k)
                E_k[n-k, n] = amplitude
            rho_out += E_k @ rho_init @ E_k.conj().T
    else:
        L_super = build_lindblad_superoperator(Gamma, n_env, a, adag, N)
        rho_vec = rho_init.reshape(N*N, order='F')
        rho_vec_final = la.expm(t * L_super) @ rho_vec
        rho_out = rho_vec_final.reshape((N, N), order='F')
    
    rho_out = 0.5 * (rho_out + rho_out.conj().T)
    trace = np.trace(rho_out)
    if trace > 1e-10:
        rho_out = rho_out / trace
    
    return rho_out

def get_prior(Gamma_grid, prior_type, Gamma0, Gamma_sigma, Gamma_min, Gamma_max):
    """Generate different types of priors on the damping rate grid"""
    if prior_type == 'gaussian':
        prior_unnorm = np.exp(-0.5 * ((Gamma_grid - Gamma0) / Gamma_sigma)**2)

    elif prior_type == 'log_normal':
        prior_unnorm = np.exp(-0.5 * ((np.log(Gamma_grid) - Gamma0) / Gamma_sigma)**2)

    elif prior_type == 'beta':
        x_scaled = (Gamma_grid - Gamma_min) / (Gamma_max - Gamma_min)
        mu_scaled = (Gamma0 - Gamma_min) / (Gamma_max - Gamma_min)
        mu_scaled = np.clip(mu_scaled, 0.01, 0.99)
        var_scaled = (Gamma_sigma / (Gamma_max - Gamma_min))**2
        var_scaled = np.clip(var_scaled, 1e-6, mu_scaled * (1 - mu_scaled) * 0.99)
        common = mu_scaled * (1 - mu_scaled) / var_scaled - 1
        alpha = max(0.5, mu_scaled * common)
        beta = max(0.5, (1 - mu_scaled) * common)
        with np.errstate(divide='ignore', invalid='ignore'):
            log_prior = (alpha - 1) * np.log(x_scaled + 1e-100) + (beta - 1) * np.log(1 - x_scaled + 1e-100)
            prior_unnorm = np.exp(log_prior)
            prior_unnorm[x_scaled <= 0] = 0
            prior_unnorm[x_scaled >= 1] = 0
            prior_unnorm = np.nan_to_num(prior_unnorm, nan=0.0, posinf=0.0, neginf=0.0)
    elif prior_type == 'gamma':
        k = (Gamma0 / Gamma_sigma)**2
        theta = Gamma_sigma**2 / Gamma0
        k = max(0.5, k)
        theta = max(1e-6, theta)
        with np.errstate(over='ignore', under='ignore', invalid='ignore'):
            log_prior = (k - 1) * np.log(Gamma_grid + 1e-100) - Gamma_grid / theta
            prior_unnorm = np.exp(log_prior)
            prior_unnorm = np.nan_to_num(prior_unnorm, nan=0.0, posinf=0.0, neginf=0.0)
    else:
        raise ValueError(f"Unknown prior type: {prior_type}")
    
    dGamma = Gamma_grid[1] - Gamma_grid[0]
    total = np.sum(prior_unnorm) * dGamma
    if total > 1e-100:
        prior = prior_unnorm / total
    else:
        print(f"Warning: Prior normalization failed for {prior_type}, using uniform")
        prior = np.ones_like(Gamma_grid) / (Gamma_max - Gamma_min)
    
    return prior

def get_optimal_coefficients(barrho, W, B):
    """Compute optimal coefficients alpha^opt for the ansatz"""
    def HS(A, Bop):
        return np.real(np.trace(A.conj().T @ Bop))
    
    m = len(B)
    G = np.zeros((m, m), dtype=float)
    b = np.zeros(m, dtype=float)
    
    for i in range(m):
        for j in range(m):
            G[i, j] = 0.5 * HS(B[i], barrho @ B[j] + B[j] @ barrho)
        b[i] = HS(B[i], W)
    
    alpha_opt, *_ = la.lstsq(G, b)
    return alpha_opt, G, b

def compute_msl_all_ansatze(Gamma_sigma, Gamma0=0.5, t=1.0, prior_type='gaussian', estimand='log'):
    """Compute MSL for Bayes-optimal and all three ansätze.
    
    Parameters:
    - estimand: 'log' to estimate f(Γ) = log(Γ), 'identity' to estimate f(Γ) = Γ
    """
    
    Gamma_grid = np.linspace(Gamma_min, Gamma_max, Gamma_pts)
    dGamma = Gamma_grid[1] - Gamma_grid[0]
    
    prior = get_prior(Gamma_grid, prior_type, Gamma0, Gamma_sigma, Gamma_min, Gamma_max)
    
    rho_init = initial_state(initial_state_type, alpha=alpha_coherent, 
                             n_th=n_thermal_init, r=r_squeeze_init, phi=phi_squeeze_init)
    
    rho_list = [amplitude_damping_channel(rho_init, Gamma, t, n_thermal_env) 
                for Gamma in Gamma_grid]
    
    # Choose estimand function
    if estimand == 'log':
        f_values = np.log(Gamma_grid)
    elif estimand == 'identity':
        f_values = Gamma_grid
    else:
        raise ValueError(f"Unknown estimand: {estimand}. Use 'log' or 'identity'")
    
    # Compute bar-rho and W
    barrho = sum(prior[i] * rho_list[i] * dGamma for i in range(len(Gamma_grid)))
    W = sum(prior[i] * f_values[i] * rho_list[i] * dGamma for i in range(len(Gamma_grid)))
    barrho = 0.5 * (barrho + barrho.conj().T)
    W = 0.5 * (W + W.conj().T)
    
    lambda_val = np.sum(prior * f_values**2 * dGamma)
    
    # Bayes-optimal
    dim = N * N
    A_big = np.kron(np.eye(N), barrho) + np.kron(barrho.conj().T, np.eye(N))
    vecW = W.reshape(dim, order='F')
    vecS_bayes = la.pinv(A_big) @ (2.0 * vecW)
    S_bayes = vecS_bayes.reshape((N, N), order='F')
    S_bayes = 0.5 * (S_bayes + S_bayes.conj().T)
    msl_bayes = lambda_val - np.real(np.trace(barrho @ (S_bayes @ S_bayes)))
    
    # Linear ansatz
    B_linear = [I, x, p]
    B_linear = [0.5 * (M + M.conj().T) for M in B_linear]
    alpha_linear, G_linear, b_linear = get_optimal_coefficients(barrho, W, B_linear)
    msl_linear = lambda_val - b_linear @ la.pinv(G_linear) @ b_linear
    
    # Quadratic ansatz
    B_quad = [I, x, p, x @ x, 0.5 * (x @ p + p @ x), p @ p]
    B_quad = [0.5 * (M + M.conj().T) for M in B_quad]
    alpha_quad, G_quad, b_quad = get_optimal_coefficients(barrho, W, B_quad)
    msl_quad = lambda_val - b_quad @ la.pinv(G_quad) @ b_quad
    
    # Cubic ansatz
    x2 = x @ x
    p2 = p @ p
    xp = 0.5 * (x @ p + p @ x)
    x3 = x @ x @ x
    x2p = 0.5 * (x2 @ p + p @ x2)
    xp2 = 0.5 * (x @ p2 + p2 @ x)
    p3 = p @ p @ p
    
    B_cubic = [I, x, p, x2, xp, p2, x3, x2p, xp2, p3]
    B_cubic = [0.5 * (M + M.conj().T) for M in B_cubic]
    alpha_cubic, G_cubic, b_cubic = get_optimal_coefficients(barrho, W, B_cubic)
    msl_cubic = lambda_val - b_cubic @ la.pinv(G_cubic) @ b_cubic
    
    return (msl_bayes, msl_linear, msl_quad, msl_cubic, 
            alpha_linear, alpha_quad, alpha_cubic)

# Compute MSL for each prior variance
msl_bayes_arr = []
msl_linear_arr = []
msl_quad_arr = []
msl_cubic_arr = []
alpha_opt_linear_list = []
alpha_opt_quad_list = []
alpha_opt_cubic_list = []

print("="*70)
print(f"Estimating damping rate Γ from amplitude damping channel")
print(f"Estimand: f(Γ) = {estimand}(Γ)")
print(f"Initial state: {initial_state_type}")
if initial_state_type == 'coherent':
    print(f"  α = {alpha_coherent}")
elif initial_state_type == 'thermal':
    print(f"  n̄_init = {n_thermal_init}")
elif initial_state_type == 'squeezed_thermal':
    print(f"  n̄_init = {n_thermal_init}, r = {r_squeeze_init}, φ = {phi_squeeze_init}")
print(f"Evolution time: t = {t_evolution}")
print(f"Environment temperature: n̄_env = {n_thermal_env}")
print(f"Prior type: {prior_type}")
print(f"Prior center: Γ₀ = {Gamma0}")
print("="*70)

for i, sigma2 in enumerate(prior_variance_list):
    Gamma_sigma = np.sqrt(sigma2)
    print(f"Progress: {i+1}/{len(prior_variance_list)}, σ² = {sigma2:.4f}", end='')
    
    results = compute_msl_all_ansatze(
        Gamma_sigma, Gamma0=Gamma0, t=t_evolution, prior_type=prior_type, estimand=estimand
    )
    
    msl_b, msl_l, msl_q, msl_c, alpha_l, alpha_q, alpha_c = results
    
    msl_bayes_arr.append(msl_b)
    msl_linear_arr.append(msl_l)
    msl_quad_arr.append(msl_q)
    msl_cubic_arr.append(msl_c)
    alpha_opt_linear_list.append(alpha_l)
    alpha_opt_quad_list.append(alpha_q)
    alpha_opt_cubic_list.append(alpha_c)
    
    print(f" -> Bayes={msl_b:.4e}, Linear={msl_l:.4e}, Quad={msl_q:.4e}, Cubic={msl_c:.4e}")

# Convert to arrays
msl_bayes_arr = np.array(msl_bayes_arr)
msl_linear_arr = np.array(msl_linear_arr)
msl_quad_arr = np.array(msl_quad_arr)
msl_cubic_arr = np.array(msl_cubic_arr)

# Plotting
fig = plt.figure(figsize=(14, 10))
gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

# Plot 1: MSL vs prior variance
ax1 = fig.add_subplot(gs[0, 0])
ax1.loglog(prior_variance_list, msl_bayes_arr, 'o-', linewidth=2.5, 
           markersize=8, label='Bayes-optimal', color='C0')
ax1.loglog(prior_variance_list, msl_linear_arr, 'd--', linewidth=2, 
           markersize=7, label='Linear SPM', color='C3')
ax1.loglog(prior_variance_list, msl_quad_arr, 's--', linewidth=2, 
           markersize=7, label='Quadratic SPM', color='C1')
ax1.loglog(prior_variance_list, msl_cubic_arr, '^:', linewidth=2, 
           markersize=7, label='Cubic SPM', color='C2')
ax1.set_xlabel('Prior variance σ²', fontsize=11)
ax1.set_ylabel('Minimum MSL (MSE)', fontsize=11)
ax1.legend(fontsize=9)

# Plot 2: Ratio to Bayes-optimal
ax2 = fig.add_subplot(gs[0, 1])
ratio_linear = msl_linear_arr / msl_bayes_arr
ratio_quad = msl_quad_arr / msl_bayes_arr
ratio_cubic = msl_cubic_arr / msl_bayes_arr

ax2.axhline(y=1, color='C0', linestyle='-', linewidth=2, alpha=0.5, label='Bayes (ratio=1)')
ax2.semilogx(prior_variance_list, ratio_linear, 'd--', linewidth=2, 
             markersize=7, label='Linear / Bayes', color='C3')
ax2.semilogx(prior_variance_list, ratio_quad, 's--', linewidth=2, 
             markersize=7, label='Quadratic / Bayes', color='C1')
ax2.semilogx(prior_variance_list, ratio_cubic, '^:', linewidth=2, 
             markersize=7, label='Cubic / Bayes', color='C2')
ax2.set_xlabel('Prior variance σ²', fontsize=11)
ax2.set_ylabel('MSL Ratio', fontsize=11)
ax2.legend(fontsize=9)
ax2.grid(False)

# Plot 3: Linear coefficients vs prior variance
ax3 = fig.add_subplot(gs[1, 0])
linear_labels = ['I', 'x', 'p']
alpha_linear_array = np.array(alpha_opt_linear_list)
for i in range(alpha_linear_array.shape[1]):
    ax3.semilogx(prior_variance_list, alpha_linear_array[:, i], 'o-', 
                 linewidth=2, markersize=5, label=linear_labels[i])
ax3.set_xlabel('Prior variance σ²', fontsize=11)
ax3.set_ylabel('Optimal coefficient α', fontsize=11)
ax3.legend(fontsize=9)
ax3.grid(False)

# Plot 4: Quadratic coefficients vs prior variance
ax4 = fig.add_subplot(gs[1, 1])
quad_labels = ['I', 'x', 'p', 'x²', '(xp+px)/2', 'p²']
alpha_quad_array = np.array(alpha_opt_quad_list)
for i in range(alpha_quad_array.shape[1]):
    ax4.semilogx(prior_variance_list, alpha_quad_array[:, i], 'o-', 
                 linewidth=2, markersize=5, label=quad_labels[i])
ax4.set_xlabel('Prior variance σ²', fontsize=11)
ax4.set_ylabel('Optimal coefficient α', fontsize=11)
ax4.legend(fontsize=9, ncol=2)
ax4.grid(False)

plt.tight_layout()
plt.show()

# Print summary
print("\n" + "="*70)
print("Summary:")
print("="*70)
print(f"Estimand: f(Γ) = {estimand}(Γ)")
print(f"Initial state: {initial_state_type}")
print(f"Evolution time: t = {t_evolution}")
print(f"Environment: n̄_env = {n_thermal_env}")
print(f"Prior type: {prior_type}")
print(f"Prior center: Γ₀ = {Gamma0}")
print(f"Prior variance range: σ² ∈ [{prior_variance_list[0]:.3f}, {prior_variance_list[-1]:.3f}]")
print(f"\nFinal MSL values (at σ² = {prior_variance_list[-1]:.3f}):")
print(f"  Bayes-optimal: {msl_bayes_arr[-1]:.6e}")
print(f"  Linear SPM:    {msl_linear_arr[-1]:.6e}")
print(f"  Quadratic SPM: {msl_quad_arr[-1]:.6e}")
print(f"  Cubic SPM:     {msl_cubic_arr[-1]:.6e}")
print(f"\nRatios to Bayes-optimal:")
print(f"  Linear / Bayes:    {ratio_linear[-1]:.4f}")
print(f"  Quadratic / Bayes: {ratio_quad[-1]:.4f}")
print(f"  Cubic / Bayes:     {ratio_cubic[-1]:.4f}")

print(f"\nFinal optimal coefficients α:")
basis_labels_quad = ['I', 'x', 'p', 'x²', '(xp+px)/2', 'p²']
for i, label in enumerate(basis_labels_quad):
    print(f"  α[{label}] = {alpha_opt_quad_list[-1][i]:+.6f}")